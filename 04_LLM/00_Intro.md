# Large Language Models (LLMs)

- GPT: Generative Pre-trained Transformer.

- Deep Learning Model trained to understand and generate natural language.

- "Attention is all you need" paper and transformer architecture.

- Prediction of next token in the sequence.

- LLMs require GPU because they keep iterating and predicting.

## Tokenization

- Sequence of numbers for each word and subwords.

- Input text is converted to tokens and fed to LLM.

- LLM takes the input tokens and predicts the next token in the sequence and this process repeats.

- Then the tokens generated by LLMs are de-tokenized and returned to user.

- Tiktokenizer: online tool to visualize tokens of different models.

- `pip install tiktoken`

## Vector Embeddings

- Semantic Meaning to tokens.
- Mathematical representations of data points like words, images, audio - as array of numbers (vectors) in a higher dimensional space.
- Checkout `project.tensorflow.org`

## Step involved

- Step-1: Tokenization
- Step-2: Vector Embeddings
- Step-3: Positional Encoding
- Step-4: Self Attention